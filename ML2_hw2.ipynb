{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Копия блокнота \"homework-practice-08-random-features.ipynb\"",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYp0bXOFK-hP"
      },
      "source": [
        "# Машинное обучение, ФКН ВШЭ\n",
        "\n",
        "## Практическое задание 8. Метод опорных векторов и аппроксимация ядер\n",
        "\n",
        "### Общая информация\n",
        "Дата выдачи: 1.10.2021\n",
        "\n",
        "Мягкий дедлайн: 17.10.2021 23:59 МСК\n",
        "\n",
        "Жесткий дедлайн: 24.10.2021 23:59 МСК (1 неделя -- минус балл)\n",
        "\n",
        "### Оценивание и штрафы\n",
        "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимальная оценка за работу (без учёта бонусов) — 10 баллов.\n",
        "\n",
        "Сдавать задание после указанного жёсткого срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
        "\n",
        "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
        "\n",
        "Неэффективная реализация кода может негативно отразиться на оценке.\n",
        "\n",
        "### Формат сдачи\n",
        "Загрузите решение в свой репозиторий на github и поделитесь [ссылкой на решение в форме](https://forms.gle/ZzCaqRj6bmfpSpyL7). Не забудьте дать доступ к Вашему репозиторию, что у преподавателей была возмоожность проверить работу."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY8vT0W_K-hR"
      },
      "source": [
        "### О задании\n",
        "\n",
        "На занятиях мы подробно обсуждали метод опорных векторов (SVM). В базовой версии в нём нет чего-то особенного — мы всего лишь используем специальную функцию потерь, которая не требует устремлять отступы к бесконечности; ей достаточно, чтобы отступы были не меньше +1. Затем мы узнали, что SVM можно переписать в двойственном виде, который, позволяет заменить скалярные произведения объектов на ядра. Это будет соответствовать построению модели в новом пространстве более высокой размерности, координаты которого представляют собой нелинейные модификации исходных признаков.\n",
        "\n",
        "Ядровой SVM, к сожалению, довольно затратен по памяти (нужно хранить матрицу Грама размера $d \\times d$) и по времени (нужно решать задачу условной оптимизации с квадратичной функцией, а это не очень быстро). Мы обсуждали, что есть способы посчитать новые признаки $\\tilde \\varphi(x)$ на основе исходных так, что скалярные произведения этих новых $\\langle \\tilde \\varphi(x), \\tilde \\varphi(z) \\rangle$ приближают ядро $K(x, z)$.\n",
        "\n",
        "Мы будем исследовать аппроксимации методом Random Fourier Features (RFF, также в литературе встречается название Random Kitchen Sinks) для гауссовых ядер. Будем использовать формулы, которые немного отличаются от того, что было на лекциях (мы добавим сдвиги внутрь тригонометрических функций и будем использовать только косинусы, потому что с нужным сдвигом косинус превратится в синус):\n",
        "$$\\tilde \\varphi(x) = (\n",
        "\\cos (w_1^T x + b_1),\n",
        "\\dots,\n",
        "\\cos (w_n^T x + b_n)\n",
        "),$$\n",
        "где $w_j \\sim \\mathcal{N}(0, 1/\\sigma^2)$, $b_j \\sim U[-\\pi, \\pi]$.\n",
        "\n",
        "На новых признаках $\\tilde \\varphi(x)$ мы будем строить любую линейную модель.\n",
        "\n",
        "Можно считать, что это некоторая новая парадигма построения сложных моделей. Можно направленно искать сложные нелинейные закономерности в данных с помощью градиентного бустинга или нейронных сетей, а можно просто нагенерировать большое количество случайных нелинейных признаков и надеяться, что быстрая и простая модель (то есть линейная) сможет показать на них хорошее качество. В этом задании мы изучим, насколько работоспособна такая идея.\n",
        "\n",
        "### Алгоритм\n",
        "\n",
        "Вам потребуется реализовать следующий алгоритм:\n",
        "1. Понизить размерность выборки до new_dim с помощью метода главных компонент.\n",
        "2. Для полученной выборки оценить гиперпараметр $\\sigma^2$ с помощью эвристики (рекомендуем считать медиану не по всем парам объектов, а по случайному подмножеству из где-то миллиона пар объектов): $$\\sigma^2 = \\text{median}_{i, j = 1, \\dots, \\ell, i \\neq j} \\left\\{\\sum_{k = 1}^{d} (x_{ik} - x_{jk})^2 \\right\\}$$\n",
        "3. Сгенерировать n_features наборов весов $w_j$ и сдвигов $b_j$.\n",
        "4. Сформировать n_features новых признаков по формулам, приведённым выше.\n",
        "5. Обучить линейную модель (логистическую регрессию или SVM) на новых признаках.\n",
        "6. Повторить преобразования (PCA, формирование новых признаков) к тестовой выборке и применить модель."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_sGunb7K-hS"
      },
      "source": [
        "Тестировать алгоритм мы будем на данных Fashion MNIST. Ниже код для их загрузки и подготовки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyG6dBfjK-hS"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "(x_train_pics, y_train), (x_test_pics, y_test) = fashion_mnist.load_data()\n",
        "x_train = x_train_pics.reshape(x_train_pics.shape[0], -1)\n",
        "x_test = x_test_pics.reshape(x_test_pics.shape[0], -1)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJNN55F7K-hT"
      },
      "source": [
        "__Задание 1. (5 баллов)__\n",
        "\n",
        "Реализуйте алгоритм, описанный выше. Можете воспользоваться шаблоном класса ниже или написать свой интерфейс.\n",
        "\n",
        "Ваша реализация должна поддерживать следующие опции:\n",
        "1. Возможность задавать значения гиперпараметров new_dim (по умолчанию 50) и n_features (по умолчанию 1000).\n",
        "2. Возможность включать или выключать предварительное понижение размерности с помощью метода главных компонент.\n",
        "3. Возможность выбирать тип линейной модели (логистическая регрессия или SVM с линейным ядром).\n",
        "\n",
        "Протестируйте на данных Fashion MNIST, сформированных кодом выше. Если на тесте у вас получилась доля верных ответов не ниже 0.84 с гиперпараметрами по умолчанию, то вы всё сделали правильно."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-VQ3cWac53A"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP8yepx8K-hT"
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "class RFFPipeline(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg'):\n",
        "        \"\"\"        \n",
        "        Implements pipeline, which consists of PCA decomposition,\n",
        "        Random Fourier Features approximation and linear classification model.\n",
        "        \n",
        "        n_features, int: amount of synthetic random features generated with RFF approximation.\n",
        "\n",
        "        new_dim, int: PCA output size.\n",
        "        \n",
        "        use_PCA, bool: whether to include PCA preprocessing.\n",
        "        \n",
        "        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n",
        "        \n",
        "        Feel free to edit this template for your preferences.    \n",
        "        \"\"\"\n",
        "        self.n_features = n_features\n",
        "        self.use_PCA = use_PCA\n",
        "        self.new_dim = new_dim\n",
        "        self.classifier = classifier\n",
        "        self.scaler1 = StandardScaler()\n",
        "        self.scaler2 = StandardScaler()\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n",
        "        \"\"\"\n",
        "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "        X_copy = self.scaler1.fit_transform(X)\n",
        "        if self.use_PCA:\n",
        "          self.pca = PCA(n_components=self.new_dim, random_state = 42)\n",
        "          X_copy = self.pca.fit_transform(X)\n",
        "\n",
        "        self.W, self.b = self._find_W_b(X_copy)\n",
        "        X_copy = self.scaler2.fit_transform(np.cos(X_copy @ self.W + self.b))\n",
        "              \n",
        "        # LinearModel\n",
        "        if self.classifier == 'svm':\n",
        "            self.clf = SVC(kernel='linear', probability=True, random_state=42)\n",
        "        elif self.classifier == 'logreg':\n",
        "            self.clf = LogisticRegression(n_jobs=-1, random_state=42)\n",
        "        else:\n",
        "            raise NotImplementedError('{} is not supported'.format(self.classifier))\n",
        "        self.clf.fit(X_copy, y)\n",
        "        return self\n",
        "\n",
        "    def _find_W_b(self, X): \n",
        "        i = np.random.choice(X.shape[0], 2000)\n",
        "        i = np.unique(i)\n",
        "        j = np.random.choice(list(set([k for k in range(X.shape[0])]) - set(i)), 2000) \n",
        "        j = np.unique(j)\n",
        "        dist = cdist(X[i], X[j]).flatten() \n",
        "        sigma = np.median(dist)\n",
        "        W = np.random.normal(0, 1/sigma, size=(X.shape[1], self.n_features))\n",
        "        b = np.random.uniform(-np.pi, np.pi, size=self.n_features)\n",
        "        return W, b\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain scores for input data.\n",
        "        \"\"\"\n",
        "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "        X_copy = self.scaler1.transform(X)\n",
        "        if self.use_PCA:\n",
        "            X_copy = self.pca.transform(X)\n",
        "        X_copy = self.scaler2.transform(np.cos(X_copy @ self.W + self.b))\n",
        "        preds_proba = self.clf.predict_proba(X_copy)\n",
        "        return preds_proba\n",
        "                \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain discrete predictions for input data.\n",
        "        \"\"\"\n",
        "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "        preds_proba = self.predict_proba(X)\n",
        "        preds = np.argmax(preds_proba, axis=1)\n",
        "        return preds\n",
        "#Частично заимствовано из\n",
        "#https://github.com/SamburskyAlexander/ML-DL/blob/6833eb39b43e5f2c30e83cb931278c03d4c2ff11/ML_fragments.ipynb"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CdJ2f8I3Cix",
        "outputId": "72c92d17-452c-4e25-9e45-2dc7e0da3b89"
      },
      "source": [
        "%%time\n",
        "RFFPipe = RFFPipeline()\n",
        "RFFPipe.fit(x_train, y_train)\n",
        "\n",
        "preds_probs = RFFPipe.predict_proba(x_test)\n",
        "preds_rff_logreg = np.argmax(preds_probs, axis=1)\n",
        "print('RFF LogReg accuracy =', round(accuracy_score(y_test, preds_rff_logreg), 4))\n",
        "print('*'*50)\n",
        "print(classification_report(y_test, preds_rff_logreg))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RFF LogReg accuracy = 0.8741\n",
            "**************************************************\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.82      0.82      1000\n",
            "           1       0.99      0.97      0.98      1000\n",
            "           2       0.78      0.78      0.78      1000\n",
            "           3       0.87      0.89      0.88      1000\n",
            "           4       0.79      0.81      0.80      1000\n",
            "           5       0.96      0.95      0.95      1000\n",
            "           6       0.69      0.66      0.67      1000\n",
            "           7       0.93      0.95      0.94      1000\n",
            "           8       0.97      0.96      0.97      1000\n",
            "           9       0.95      0.95      0.95      1000\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n",
            "CPU times: user 16.7 s, sys: 4.57 s, total: 21.2 s\n",
            "Wall time: 1min 10s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07on_t5eAGpc"
      },
      "source": [
        "RFFPipe = RFFPipeline(classifier='svm')\n",
        "RFFPipe.fit(x_train, y_train)\n",
        "\n",
        "preds_probs = RFFPipe.predict_proba(x_test)\n",
        "preds_rff_svm = np.argmax(preds_probs, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQSupa9NnNBm",
        "outputId": "4d7b6d96-7f68-4848-8603-19ed770e56ed"
      },
      "source": [
        "print('RFF SVM accuracy =', round(accuracy_score(y_test, preds_rff_svm), 4))\n",
        "print('*'*50)\n",
        "print(classification_report(y_test, preds_rff_svm))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RFF SVM accuracy = 0.8807\n",
            "**************************************************\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.85      0.83      1000\n",
            "           1       0.99      0.96      0.98      1000\n",
            "           2       0.78      0.80      0.79      1000\n",
            "           3       0.88      0.89      0.89      1000\n",
            "           4       0.81      0.81      0.81      1000\n",
            "           5       0.96      0.95      0.96      1000\n",
            "           6       0.72      0.67      0.69      1000\n",
            "           7       0.93      0.96      0.94      1000\n",
            "           8       0.97      0.97      0.97      1000\n",
            "           9       0.95      0.95      0.95      1000\n",
            "\n",
            "    accuracy                           0.88     10000\n",
            "   macro avg       0.88      0.88      0.88     10000\n",
            "weighted avg       0.88      0.88      0.88     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqXv29-h9HRS"
      },
      "source": [
        "SVM дает небольшой прирост в качестве, но работает намного медленнеее, чем LogReg."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYqQUEi-K-hU"
      },
      "source": [
        "__Задание 2. (3 балла)__\n",
        "\n",
        "Сравните подход со случайными признаками с обучением SVM на исходных признаках. Попробуйте вариант с обычным (линейным) SVM и с ядровым SVM. Ядровой SVM может очень долго обучаться, поэтому можно делать любые разумные вещи для ускорения: брать подмножество объектов из обучающей выборки, например.\n",
        "\n",
        "Сравните подход со случайными признаками с вариантом, в котором вы понижаете размерность с помощью PCA и обучаете градиентный бустинг. Используйте одну из реализаций CatBoost/LightGBM/XGBoost, не забудьте подобрать число деревьев и длину шага.\n",
        "\n",
        "Сделайте выводы — насколько идея со случайными признаками работает? Сравните как с точки зрения качества, так и с точки зрения скорости обучения и применения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN8LUlJgK-hV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07a4b832-448f-4081-b35a-545739c03c71"
      },
      "source": [
        "# Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "#Линейный SVM на подвыборке из исходных признаков\n",
        "%%time\n",
        "random_obj = list(set(np.random.choice(x_train.shape[0], 10000)))\n",
        "x_train_random = x_train[random_obj]\n",
        "y_train_random = y_train[random_obj]\n",
        "LinearSVM = SVC(kernel='linear', probability=True, random_state=42)\n",
        "LinearSVM.fit(x_train_random, y_train_random)\n",
        "\n",
        "preds_probs = LinearSVM.predict_proba(x_test)\n",
        "preds_linear_SVM = np.argmax(preds_probs, axis=1)\n",
        "print('Linear SVM accuracy =', round(accuracy_score(y_test, preds_linear_SVM), 4))\n",
        "print('*'*50)\n",
        "print(classification_report(y_test, preds_linear_SVM))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVM accuracy = 0.8029\n",
            "**************************************************\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.76      0.76      1000\n",
            "           1       0.97      0.95      0.96      1000\n",
            "           2       0.65      0.64      0.64      1000\n",
            "           3       0.81      0.84      0.83      1000\n",
            "           4       0.67      0.67      0.67      1000\n",
            "           5       0.91      0.89      0.90      1000\n",
            "           6       0.53      0.51      0.52      1000\n",
            "           7       0.87      0.90      0.89      1000\n",
            "           8       0.93      0.94      0.93      1000\n",
            "           9       0.92      0.93      0.93      1000\n",
            "\n",
            "    accuracy                           0.80     10000\n",
            "   macro avg       0.80      0.80      0.80     10000\n",
            "weighted avg       0.80      0.80      0.80     10000\n",
            "\n",
            "CPU times: user 3min 5s, sys: 136 ms, total: 3min 5s\n",
            "Wall time: 3min 4s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xuAKpK77XyE",
        "outputId": "eb41dc0b-b581-4fc1-d642-db32ece6f8ae"
      },
      "source": [
        "#Ядровой SVM на подвыборке из исходных признаков\n",
        "%%time\n",
        "KernelSVM = SVC(kernel='poly', probability=True, random_state=42, degree=10)\n",
        "KernelSVM.fit(x_train_random, y_train_random)\n",
        "\n",
        "preds_pobs = KernelSVM.predict_proba(x_test)\n",
        "preds_kernel_svm = np.argmax(preds_probs, axis=1)\n",
        "print('Kernel SVM accuracy =', round(accuracy_score(y_test, preds_kernel_svm), 4))\n",
        "print('*'*50)\n",
        "print(classification_report(y_test, preds_kernel_svm))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kernel SVM accuracy = 0.8741\n",
            "**************************************************\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.82      0.82      1000\n",
            "           1       0.99      0.97      0.98      1000\n",
            "           2       0.78      0.78      0.78      1000\n",
            "           3       0.87      0.89      0.88      1000\n",
            "           4       0.79      0.81      0.80      1000\n",
            "           5       0.96      0.95      0.95      1000\n",
            "           6       0.69      0.66      0.67      1000\n",
            "           7       0.93      0.95      0.94      1000\n",
            "           8       0.97      0.96      0.97      1000\n",
            "           9       0.95      0.95      0.95      1000\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n",
            "CPU times: user 5min 18s, sys: 325 ms, total: 5min 18s\n",
            "Wall time: 5min 17s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E53M4WKf6Ljn",
        "outputId": "6275bf71-eda7-4d44-dfaf-1b51681ebff9"
      },
      "source": [
        "#RFF SVM на подвыборке из исходных признаков\n",
        "%%time\n",
        "RFFPipe = RFFPipeline(classifier='svm')\n",
        "RFFPipe.fit(x_train_random, y_train_random)\n",
        "\n",
        "preds_probs = RFFPipe.predict_proba(x_test)\n",
        "preds_rff_svm = np.argmax(preds_probs, axis=1)\n",
        "print('RFF SVM accuracy =', round(accuracy_score(y_test, preds_rff_svm), 4))\n",
        "print('*'*50)\n",
        "print(classification_report(y_test, preds_rff_svm))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RFF SVM accuracy = 0.8457\n",
            "**************************************************\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.84      0.78      1000\n",
            "           1       0.98      0.96      0.97      1000\n",
            "           2       0.75      0.75      0.75      1000\n",
            "           3       0.86      0.86      0.86      1000\n",
            "           4       0.74      0.75      0.74      1000\n",
            "           5       0.96      0.93      0.94      1000\n",
            "           6       0.66      0.56      0.61      1000\n",
            "           7       0.90      0.93      0.91      1000\n",
            "           8       0.96      0.96      0.96      1000\n",
            "           9       0.93      0.93      0.93      1000\n",
            "\n",
            "    accuracy                           0.85     10000\n",
            "   macro avg       0.85      0.85      0.84     10000\n",
            "weighted avg       0.85      0.85      0.84     10000\n",
            "\n",
            "CPU times: user 3min 5s, sys: 1.69 s, total: 3min 6s\n",
            "Wall time: 3min 4s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ-51Lbt9Xuq"
      },
      "source": [
        "На подвыборке из 10000 ядровой SVM показал лучшее качество, чем SVM на случайных признаках при сравнимой скорости обучения. При других запусках лучшее качество чаще всего показывал RFF SVM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KxpGAAFEuAK"
      },
      "source": [
        "from lightgbm import LGBMClassifier"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVeOIwpoD8lv",
        "outputId": "786e2593-51b2-4482-cec8-31374f86a181"
      },
      "source": [
        "%%time\n",
        "model = LGBMClassifier(max_depth=7, learning_rate=0.2)\n",
        "pca = PCA(n_components=50, random_state=42)\n",
        "X_copy = pca.fit_transform(x_train)\n",
        "model.fit(X_copy, y_train)\n",
        "\n",
        "X_test_copy = pca.transform(x_test)\n",
        "preds_ = model.predict_proba(X_test_copy)\n",
        "preds_lgbm = np.argmax(preds_, axis=1)\n",
        "print('LGBM accuracy =', round(accuracy_score(y_test, preds_lgbm), 4))\n",
        "print('*'*50)\n",
        "print(classification_report(y_test, preds_lgbm))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGBM accuracy = 0.8683\n",
            "**************************************************\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.84      0.83      1000\n",
            "           1       0.99      0.96      0.97      1000\n",
            "           2       0.76      0.78      0.77      1000\n",
            "           3       0.88      0.89      0.89      1000\n",
            "           4       0.78      0.80      0.79      1000\n",
            "           5       0.95      0.94      0.95      1000\n",
            "           6       0.66      0.61      0.64      1000\n",
            "           7       0.92      0.94      0.93      1000\n",
            "           8       0.96      0.96      0.96      1000\n",
            "           9       0.94      0.95      0.95      1000\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n",
            "CPU times: user 58.4 s, sys: 2.71 s, total: 1min 1s\n",
            "Wall time: 31.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg9zCzLA9-0y"
      },
      "source": [
        "Качество, полученное для градиентного бустинга при подобранных гиперпараметрах всего на 0.01 хуже, чем качество, полученное для логистической регрессии и SVM со случайными признаками. Но SVM на случайных признаках обучался 1,5 часа. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnegLSwXBKaS"
      },
      "source": [
        "Скорость обучения градиентного бустинга в 2 раза меньше, чем скорость обучения логистической регрессии, но при этом обе модели обучаются быстро. Таким образом, модель градиентного бустинга на исходных признаках с применением PCA может быть альтернативой логистической регрессии на случайных признаках."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6umjhWuK-hV"
      },
      "source": [
        "__Задание 3. (2 балла)__\n",
        "\n",
        "Проведите эксперименты:\n",
        "1. Помогает ли предварительное понижение размерности с помощью PCA? \n",
        "2. Как зависит итоговое качество от n_features? Выходит ли оно на плато при росте n_features?\n",
        "3. Важно ли, какую модель обучать — логистическую регрессию или SVM?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2QIHIMbK-hW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c95efd31-7d9f-4014-fd0b-b63669492282"
      },
      "source": [
        "# Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "RFFPipe = RFFPipeline(use_PCA=False)\n",
        "RFFPipe.fit(x_train, y_train)\n",
        "\n",
        "preds_probs = RFFPipe.predict_proba(x_test)\n",
        "preds_rff_no_PCA = np.argmax(preds_probs, axis=1)\n",
        "print('RFF LogReg accuracy =', round(accuracy_score(y_test, preds_rff_no_PCA), 4))\n",
        "print('*'*50)\n",
        "print(classification_report(y_test, preds_rff_no_PCA))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RFF LogReg accuracy = 0.8592\n",
            "**************************************************\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.80      0.81      1000\n",
            "           1       0.98      0.96      0.97      1000\n",
            "           2       0.76      0.76      0.76      1000\n",
            "           3       0.84      0.88      0.86      1000\n",
            "           4       0.77      0.79      0.78      1000\n",
            "           5       0.94      0.94      0.94      1000\n",
            "           6       0.66      0.64      0.65      1000\n",
            "           7       0.93      0.95      0.94      1000\n",
            "           8       0.94      0.94      0.94      1000\n",
            "           9       0.94      0.94      0.94      1000\n",
            "\n",
            "    accuracy                           0.86     10000\n",
            "   macro avg       0.86      0.86      0.86     10000\n",
            "weighted avg       0.86      0.86      0.86     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7BwwOnSb-m9",
        "outputId": "75fb9e31-f054-4873-cea7-5101b167cacc"
      },
      "source": [
        "RFFPipe = RFFPipeline(use_PCA=False, classifier='svm')\n",
        "RFFPipe.fit(x_train_random, y_train_random)\n",
        "\n",
        "preds_probs = RFFPipe.predict_proba(x_test)\n",
        "preds_rff_svm = np.argmax(preds_probs, axis=1)\n",
        "print('RFF SVM accuracy =', round(accuracy_score(y_test, preds_rff_svm), 4))\n",
        "print('*'*50)\n",
        "print(classification_report(y_test, preds_rff_svm))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RFF SVM accuracy = 0.8326\n",
            "**************************************************\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.81      0.78      1000\n",
            "           1       0.98      0.96      0.97      1000\n",
            "           2       0.75      0.71      0.73      1000\n",
            "           3       0.84      0.86      0.85      1000\n",
            "           4       0.72      0.75      0.73      1000\n",
            "           5       0.90      0.90      0.90      1000\n",
            "           6       0.62      0.55      0.58      1000\n",
            "           7       0.90      0.91      0.90      1000\n",
            "           8       0.93      0.95      0.94      1000\n",
            "           9       0.92      0.93      0.92      1000\n",
            "\n",
            "    accuracy                           0.83     10000\n",
            "   macro avg       0.83      0.83      0.83     10000\n",
            "weighted avg       0.83      0.83      0.83     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qegzlObPcFms"
      },
      "source": [
        "1. Если предварительно не понижать размерность при помощи PCA, то качество на несколько сотых меньше, чем при понижении размерности. То есть применение PCA помогает улучшать качество."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfbsvCF-b5hO"
      },
      "source": [
        "n_features = np.arange(100, 1600, 100)\n",
        "preds_feat = []\n",
        "for n in n_features:\n",
        "  RFFPipe = RFFPipeline(n_features=n)\n",
        "  RFFPipe.fit(x_train, y_train)\n",
        "\n",
        "  preds_probs = RFFPipe.predict_proba(x_test)\n",
        "  preds_feat.append(accuracy_score(y_test, np.argmax(preds_probs, axis=1)))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA_0KPOz3QLG"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "kn8IjKuh28Tb",
        "outputId": "ac5aa8e8-cd0b-4053-88d6-a61db47220df"
      },
      "source": [
        "plt.plot(n_features, preds_feat)\n",
        "plt.title('Качество LogReg при разном количестве n features')\n",
        "plt.xlabel('n features')\n",
        "plt.ylabel('accuracy')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyizFc7odPUE"
      },
      "source": [
        "2. С увеличением n features качество улучшается и выходит на плато при n features >= 1000. Таким образом 1000 признаков является оптимальным значением."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjGt7xeB6N91"
      },
      "source": [
        "3. В задании 1 были обучены две модели: логистическая регрессия и SVM. Качество SVM оказалось незначительно выше, чем у логистической регрессии. При этом при других запусках качество логистической регрессии может стать выше за счет других параметров w и b. Обучение SVM занимает 1,5 часа, поэтому логистическая регрессия является оптимальной моделью."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJqXVuasK-hW"
      },
      "source": [
        "### Бонус"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVDWHCdrK-hX"
      },
      "source": [
        "__Задание 4. (Максимум 2 балла)__\n",
        "\n",
        "Как вы, должно быть, помните с курса МО-1, многие алгоритмы машинного обучения работают лучше, если признаки данных некоррелированы. Оказывается, что для RFF существует модификация, позволяющая получать ортогональные случайные признаки (Orthogonal Random Features, ORF). Об этом методе можно прочитать в [статье](https://proceedings.neurips.cc/paper/2016/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf). Реализуйте класс для вычисления ORF по аналогии с основным заданием. Обратите внимание, что ваш класс должен уметь работать со случаем n_features > new_dim (в статье есть замечание на этот счет). Проведите эксперименты, сравнивающие RFF и ORF, сделайте выводы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pc7-1jmK-hY"
      },
      "source": [
        "__Задание 5. (Максимум 2 балла)__\n",
        "\n",
        "Поэкспериментируйте с функциями для вычисления новых случайных признаков. Не обязательно использовать косинус от скалярного произведения — можно брать знак от него, хэш и т.д. Придумайте побольше вариантов для генерации признаков и проверьте, не получается ли с их помощью добиваться более высокого качества. Также можете попробовать другой классификатор поверх случайных признаков, сравните результаты."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWj-O2vjK-hY"
      },
      "source": [
        "# Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP3-sMtMmgw5"
      },
      "source": [
        "class RFFPipeline(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg'):\n",
        "        \"\"\"        \n",
        "        Implements pipeline, which consists of PCA decomposition,\n",
        "        Random Fourier Features approximation and linear classification model.\n",
        "        \n",
        "        n_features, int: amount of synthetic random features generated with RFF approximation.\n",
        "\n",
        "        new_dim, int: PCA output size.\n",
        "        \n",
        "        use_PCA, bool: whether to include PCA preprocessing.\n",
        "        \n",
        "        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n",
        "        \n",
        "        Feel free to edit this template for your preferences.    \n",
        "        \"\"\"\n",
        "        self.n_features = n_features\n",
        "        self.use_PCA = use_PCA\n",
        "        self.new_dim = new_dim\n",
        "        self.classifier = classifier\n",
        "        self.scaler1 = StandardScaler()\n",
        "        self.scaler2 = StandardScaler()\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n",
        "        \"\"\"\n",
        "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "        X_copy = self.scaler1.fit_transform(X)\n",
        "        if self.use_PCA:\n",
        "          self.pca = PCA(n_components=self.new_dim, random_state = 42)\n",
        "          X_copy = self.pca.fit_transform(X)\n",
        "\n",
        "        self.W, self.b = self._find_W_b(X_copy)\n",
        "        X_copy = self.scaler2.fit_transform(np.cos(X_copy @ self.W + self.b)**3)\n",
        "              \n",
        "        # LinearModel\n",
        "        if self.classifier == 'svm':\n",
        "            self.clf = SVC(kernel='linear', probability=True, random_state=42)\n",
        "        elif self.classifier == 'logreg':\n",
        "            self.clf = LogisticRegression(n_jobs=-1, random_state=42)\n",
        "        else:\n",
        "            raise NotImplementedError('{} is not supported'.format(self.classifier))\n",
        "        self.clf.fit(X_copy, y)\n",
        "        return self\n",
        "\n",
        "    def _find_W_b(self, X): \n",
        "        i = np.random.choice(X.shape[0], 2000)\n",
        "        i = np.unique(i)\n",
        "        j = np.random.choice(list(set([k for k in range(X.shape[0])]) - set(i)), 2000) \n",
        "        j = np.unique(j)\n",
        "        dist = cdist(X[i], X[j]).flatten() \n",
        "        sigma = np.median(dist)\n",
        "        W = np.random.normal(0, 1/sigma, size=(X.shape[1], self.n_features))\n",
        "        b = np.random.uniform(-np.pi, np.pi, size=self.n_features)\n",
        "        return W, b\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain scores for input data.\n",
        "        \"\"\"\n",
        "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "        X_copy = self.scaler1.transform(X)\n",
        "        if self.use_PCA:\n",
        "            X_copy = self.pca.transform(X)\n",
        "        X_copy = self.scaler2.transform(np.cos(X_copy @ self.W + self.b))\n",
        "        preds_proba = self.clf.predict_proba(X_copy)\n",
        "        return preds_proba\n",
        "                \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain discrete predictions for input data.\n",
        "        \"\"\"\n",
        "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "        preds_proba = self.predict_proba(X)\n",
        "        preds = np.argmax(preds_proba, axis=1)\n",
        "        return preds"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yO8pT35ymgxH",
        "outputId": "feb7edbb-9089-4953-8bdb-2e862ff6f07a"
      },
      "source": [
        "#cos^3\n",
        "RFFPipe = RFFPipeline()\n",
        "RFFPipe.fit(x_train, y_train)\n",
        "\n",
        "preds_probs = RFFPipe.predict_proba(x_test)\n",
        "preds_rff_logreg = np.argmax(preds_probs, axis=1)\n",
        "print('RFF LogReg accuracy =', round(accuracy_score(y_test, preds_rff_logreg), 4))\n",
        "print('*'*50)\n",
        "print(classification_report(y_test, preds_rff_logreg))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RFF LogReg accuracy = 0.7073\n",
            "**************************************************\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.23      0.37      1000\n",
            "           1       1.00      0.89      0.94      1000\n",
            "           2       0.70      0.61      0.65      1000\n",
            "           3       0.74      0.81      0.78      1000\n",
            "           4       0.57      0.81      0.67      1000\n",
            "           5       0.79      0.92      0.85      1000\n",
            "           6       0.31      0.34      0.32      1000\n",
            "           7       0.73      0.80      0.76      1000\n",
            "           8       0.69      0.95      0.80      1000\n",
            "           9       0.97      0.72      0.82      1000\n",
            "\n",
            "    accuracy                           0.71     10000\n",
            "   macro avg       0.74      0.71      0.70     10000\n",
            "weighted avg       0.74      0.71      0.70     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEbyrjdem6uu"
      },
      "source": [
        "class RFFPipeline(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg'):\n",
        "        \"\"\"        \n",
        "        Implements pipeline, which consists of PCA decomposition,\n",
        "        Random Fourier Features approximation and linear classification model.\n",
        "        \n",
        "        n_features, int: amount of synthetic random features generated with RFF approximation.\n",
        "\n",
        "        new_dim, int: PCA output size.\n",
        "        \n",
        "        use_PCA, bool: whether to include PCA preprocessing.\n",
        "        \n",
        "        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n",
        "        \n",
        "        Feel free to edit this template for your preferences.    \n",
        "        \"\"\"\n",
        "        self.n_features = n_features\n",
        "        self.use_PCA = use_PCA\n",
        "        self.new_dim = new_dim\n",
        "        self.classifier = classifier\n",
        "        self.scaler1 = StandardScaler()\n",
        "        self.scaler2 = StandardScaler()\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n",
        "        \"\"\"\n",
        "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "        X_copy = self.scaler1.fit_transform(X)\n",
        "        if self.use_PCA:\n",
        "          self.pca = PCA(n_components=self.new_dim, random_state = 42)\n",
        "          X_copy = self.pca.fit_transform(X)\n",
        "\n",
        "        self.W, self.b = self._find_W_b(X_copy)\n",
        "        X_copy = self.scaler2.fit_transform(np.sign(X_copy @ self.W + self.b))\n",
        "              \n",
        "        # LinearModel\n",
        "        if self.classifier == 'svm':\n",
        "            self.clf = SVC(kernel='linear', probability=True, random_state=42)\n",
        "        elif self.classifier == 'logreg':\n",
        "            self.clf = LogisticRegression(n_jobs=-1, random_state=42)\n",
        "        else:\n",
        "            raise NotImplementedError('{} is not supported'.format(self.classifier))\n",
        "        self.clf.fit(X_copy, y)\n",
        "        return self\n",
        "\n",
        "    def _find_W_b(self, X): \n",
        "        i = np.random.choice(X.shape[0], 2000)\n",
        "        i = np.unique(i)\n",
        "        j = np.random.choice(list(set([k for k in range(X.shape[0])]) - set(i)), 2000) \n",
        "        j = np.unique(j)\n",
        "        dist = cdist(X[i], X[j]).flatten() \n",
        "        sigma = np.median(dist)\n",
        "        W = np.random.normal(0, 1/sigma, size=(X.shape[1], self.n_features))\n",
        "        b = np.random.uniform(-np.pi, np.pi, size=self.n_features)\n",
        "        return W, b\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain scores for input data.\n",
        "        \"\"\"\n",
        "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "        X_copy = self.scaler1.transform(X)\n",
        "        if self.use_PCA:\n",
        "            X_copy = self.pca.transform(X)\n",
        "        X_copy = self.scaler2.transform(np.cos(X_copy @ self.W + self.b))\n",
        "        preds_proba = self.clf.predict_proba(X_copy)\n",
        "        return preds_proba\n",
        "                \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain discrete predictions for input data.\n",
        "        \"\"\"\n",
        "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "        preds_proba = self.predict_proba(X)\n",
        "        preds = np.argmax(preds_proba, axis=1)\n",
        "        return preds"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSRLZF8XoazB",
        "outputId": "6c787cb4-28dd-4f0a-9a9e-c455a1471c07"
      },
      "source": [
        "#sign вместо cos\n",
        "RFFPipe = RFFPipeline()\n",
        "RFFPipe.fit(x_train, y_train)\n",
        "\n",
        "preds_probs = RFFPipe.predict_proba(x_test)\n",
        "preds_rff_logreg = np.argmax(preds_probs, axis=1)\n",
        "print('RFF LogReg accuracy =', round(accuracy_score(y_test, preds_rff_logreg), 4))\n",
        "print('*'*50)\n",
        "print(classification_report(y_test, preds_rff_logreg))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RFF LogReg accuracy = 0.0603\n",
            "**************************************************\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.02      0.00      0.00      1000\n",
            "           1       0.00      0.00      0.00      1000\n",
            "           2       0.05      0.18      0.07      1000\n",
            "           3       0.00      0.00      0.00      1000\n",
            "           4       0.00      0.00      0.00      1000\n",
            "           5       0.07      0.42      0.12      1000\n",
            "           6       0.00      0.00      0.00      1000\n",
            "           7       0.00      0.00      0.00      1000\n",
            "           8       0.00      0.00      0.00      1000\n",
            "           9       0.00      0.00      0.00      1000\n",
            "\n",
            "    accuracy                           0.06     10000\n",
            "   macro avg       0.01      0.06      0.02     10000\n",
            "weighted avg       0.01      0.06      0.02     10000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSZv6Nijp_B0"
      },
      "source": [
        "class RFFPipeline(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg'):\n",
        "        \"\"\"        \n",
        "        Implements pipeline, which consists of PCA decomposition,\n",
        "        Random Fourier Features approximation and linear classification model.\n",
        "        \n",
        "        n_features, int: amount of synthetic random features generated with RFF approximation.\n",
        "\n",
        "        new_dim, int: PCA output size.\n",
        "        \n",
        "        use_PCA, bool: whether to include PCA preprocessing.\n",
        "        \n",
        "        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n",
        "        \n",
        "        Feel free to edit this template for your preferences.    \n",
        "        \"\"\"\n",
        "        self.n_features = n_features\n",
        "        self.use_PCA = use_PCA\n",
        "        self.new_dim = new_dim\n",
        "        self.classifier = classifier\n",
        "        self.scaler1 = StandardScaler()\n",
        "        self.scaler2 = StandardScaler()\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n",
        "        \"\"\"\n",
        "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "        X_copy = self.scaler1.fit_transform(X)\n",
        "        if self.use_PCA:\n",
        "          self.pca = PCA(n_components=self.new_dim, random_state = 42)\n",
        "          X_copy = self.pca.fit_transform(X)\n",
        "\n",
        "        self.W, self.b = self._find_W_b(X_copy)\n",
        "        X_copy = self.scaler2.fit_transform(np.cos(X_copy @ self.W + self.b))\n",
        "              \n",
        "        # LinearModel\n",
        "        if self.classifier == 'svm':\n",
        "            self.clf = SVC(kernel='linear', probability=True, random_state=42)\n",
        "        elif self.classifier == 'logreg':\n",
        "            self.clf = LogisticRegression(n_jobs=-1, random_state=42)\n",
        "        else:\n",
        "            self.clf = RandomForestClassifier(random_state=42)\n",
        "        self.clf.fit(X_copy, y)\n",
        "        return self\n",
        "\n",
        "    def _find_W_b(self, X): \n",
        "        i = np.random.choice(X.shape[0], 2000)\n",
        "        i = np.unique(i)\n",
        "        j = np.random.choice(list(set([k for k in range(X.shape[0])]) - set(i)), 2000) \n",
        "        j = np.unique(j)\n",
        "        dist = cdist(X[i], X[j]).flatten() \n",
        "        sigma = np.median(dist)\n",
        "        W = np.random.normal(0, 1/sigma, size=(X.shape[1], self.n_features))\n",
        "        b = np.random.uniform(-np.pi, np.pi, size=self.n_features)\n",
        "        return W, b\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain scores for input data.\n",
        "        \"\"\"\n",
        "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "        X_copy = self.scaler1.transform(X)\n",
        "        if self.use_PCA:\n",
        "            X_copy = self.pca.transform(X)\n",
        "        X_copy = self.scaler2.transform(np.cos(X_copy @ self.W + self.b))\n",
        "        preds_proba = self.clf.predict_proba(X_copy)\n",
        "        return preds_proba\n",
        "                \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain discrete predictions for input data.\n",
        "        \"\"\"\n",
        "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "        preds_proba = self.predict_proba(X)\n",
        "        preds = np.argmax(preds_proba, axis=1)\n",
        "        return preds"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy3chFaOp_CG",
        "outputId": "9f55e19e-d197-4f04-b6d7-16fc8eb48d65"
      },
      "source": [
        "# Random rorest на случайных признаках\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "RFFPipe = RFFPipeline(classifier='RF')\n",
        "RFFPipe.fit(x_train, y_train)\n",
        "\n",
        "preds_probs = RFFPipe.predict_proba(x_test)\n",
        "preds_rff_rf = np.argmax(preds_probs, axis=1)\n",
        "print('RFF LogReg accuracy =', round(accuracy_score(y_test, preds_rff_rf), 4))\n",
        "print('*'*50)\n",
        "print(classification_report(y_test, preds_rff_rf))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RFF LogReg accuracy = 0.8622\n",
            "**************************************************\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.84      0.82      1000\n",
            "           1       0.99      0.95      0.97      1000\n",
            "           2       0.76      0.78      0.77      1000\n",
            "           3       0.86      0.89      0.88      1000\n",
            "           4       0.78      0.80      0.79      1000\n",
            "           5       0.93      0.94      0.94      1000\n",
            "           6       0.68      0.59      0.63      1000\n",
            "           7       0.92      0.93      0.92      1000\n",
            "           8       0.94      0.96      0.95      1000\n",
            "           9       0.93      0.94      0.94      1000\n",
            "\n",
            "    accuracy                           0.86     10000\n",
            "   macro avg       0.86      0.86      0.86     10000\n",
            "weighted avg       0.86      0.86      0.86     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4LAsTKttToR"
      },
      "source": [
        "class RFFPipeline(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg'):\n",
        "        \"\"\"        \n",
        "        Implements pipeline, which consists of PCA decomposition,\n",
        "        Random Fourier Features approximation and linear classification model.\n",
        "        \n",
        "        n_features, int: amount of synthetic random features generated with RFF approximation.\n",
        "\n",
        "        new_dim, int: PCA output size.\n",
        "        \n",
        "        use_PCA, bool: whether to include PCA preprocessing.\n",
        "        \n",
        "        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n",
        "        \n",
        "        Feel free to edit this template for your preferences.    \n",
        "        \"\"\"\n",
        "        self.n_features = n_features\n",
        "        self.use_PCA = use_PCA\n",
        "        self.new_dim = new_dim\n",
        "        self.classifier = classifier\n",
        "        self.scaler1 = StandardScaler()\n",
        "        self.scaler2 = StandardScaler()\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n",
        "        \"\"\"\n",
        "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "        X_copy = self.scaler1.fit_transform(X)\n",
        "        if self.use_PCA:\n",
        "          self.pca = PCA(n_components=self.new_dim, random_state = 42)\n",
        "          X_copy = self.pca.fit_transform(X)\n",
        "\n",
        "        self.W, self.b = self._find_W_b(X_copy)\n",
        "        X_copy = self.scaler2.fit_transform(np.cos(X_copy @ self.W + self.b))\n",
        "              \n",
        "        # LinearModel\n",
        "        if self.classifier == 'svm':\n",
        "            self.clf = SVC(kernel='linear', probability=True, random_state=42)\n",
        "        elif self.classifier == 'logreg':\n",
        "            self.clf = LogisticRegression(n_jobs=-1, random_state=42)\n",
        "        else:\n",
        "            self.clf = LGBMClassifier(max_depth=10, learning_rate=0.2)\n",
        "        self.clf.fit(X_copy, y)\n",
        "        return self\n",
        "\n",
        "    def _find_W_b(self, X): \n",
        "        i = np.random.choice(X.shape[0], 2000)\n",
        "        i = np.unique(i)\n",
        "        j = np.random.choice(list(set([k for k in range(X.shape[0])]) - set(i)), 2000) \n",
        "        j = np.unique(j)\n",
        "        dist = cdist(X[i], X[j]).flatten() \n",
        "        sigma = np.median(dist)\n",
        "        W = np.random.normal(0, 1/sigma, size=(X.shape[1], self.n_features))\n",
        "        b = np.random.uniform(-np.pi, np.pi, size=self.n_features)\n",
        "        return W, b\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain scores for input data.\n",
        "        \"\"\"\n",
        "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "        X_copy = self.scaler1.transform(X)\n",
        "        if self.use_PCA:\n",
        "            X_copy = self.pca.transform(X)\n",
        "        X_copy = self.scaler2.transform(np.cos(X_copy @ self.W + self.b))\n",
        "        preds_proba = self.clf.predict_proba(X_copy)\n",
        "        return preds_proba\n",
        "                \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain discrete predictions for input data.\n",
        "        \"\"\"\n",
        "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "        preds_proba = self.predict_proba(X)\n",
        "        preds = np.argmax(preds_proba, axis=1)\n",
        "        return preds"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxUn9HDGtGBa",
        "outputId": "6ab2f654-f463-47ec-9ba9-eaba4841f331"
      },
      "source": [
        "# градиентный бустинг на случайных признаках\n",
        "RFFPipe = RFFPipeline(classifier='LGBM')\n",
        "RFFPipe.fit(x_train, y_train)\n",
        "\n",
        "preds_probs = RFFPipe.predict_proba(x_test)\n",
        "preds_rff_b = np.argmax(preds_probs, axis=1)\n",
        "print('RFF LogReg accuracy =', round(accuracy_score(y_test, preds_rff_b), 4))\n",
        "print('*'*50)\n",
        "print(classification_report(y_test, preds_rff_b))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RFF LogReg accuracy = 0.8768\n",
            "**************************************************\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.83      0.83      1000\n",
            "           1       0.99      0.97      0.98      1000\n",
            "           2       0.78      0.79      0.79      1000\n",
            "           3       0.89      0.90      0.89      1000\n",
            "           4       0.78      0.81      0.80      1000\n",
            "           5       0.96      0.95      0.96      1000\n",
            "           6       0.68      0.65      0.67      1000\n",
            "           7       0.93      0.95      0.94      1000\n",
            "           8       0.97      0.97      0.97      1000\n",
            "           9       0.95      0.95      0.95      1000\n",
            "\n",
            "    accuracy                           0.88     10000\n",
            "   macro avg       0.88      0.88      0.88     10000\n",
            "weighted avg       0.88      0.88      0.88     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1232WvMcr75n"
      },
      "source": [
        "Возведение косинуса в различные степени не дает прирост качества (для примера оставила степень 3). Если брать знак скалярного произведения, то качесво близко к случайному угадыванию."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCTmomkqs0_g"
      },
      "source": [
        "Применение случайного леса в качестве классификатора дает качество хуже, чем у логистической регрессии и SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14s-uK9q4Kwl"
      },
      "source": [
        "Градиентный бустинг на случайных признаках дает качество немного выше, чем логистическая регрессия, но не дотягивает до SVM на случайных признаках."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgIClY8SqV7b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}